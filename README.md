# DOOMGAN: High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing

[![Conference](https://img.shields.io/badge/IJCB-2025-blue)](https://arxiv.org/abs/2507.17158)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Official PyTorch implementation for our paper, **"DOOMGAN: High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing"**, accepted at the IEEE International Joint Conference on Biometrics (IJCB) 2025.

**Authors:** Bharath Krishnamurthy and Ajita Rattani  
*University of North Texas*

---

<p align="center">
  <img src="assets/eye_morphing_comparison.png" width="800" alt="DOOMGAN Teaser">
  <br>
  <em>Figure 1: Illustration of morphed ocular images generated by our proposed DOOMGAN. A morphed image combines images of two different identities such that the ensuing biometric template can match both contributing subjects, posing a serious security threat.</em>
</p>

## Abstract

> Ocular biometrics in the visible spectrum have emerged as a prominent modality due to their high accuracy, resistance to spoofing, and non-invasive nature. Morphing attacks—where synthetic biometric traits are created by merging features from multiple individuals—pose a significant threat to the trustworthiness and integrity of biometric systems. While morphing attacks have been thoroughly investigated for the near-infrared iris and face biometrics, their implications for visible-spectrum ocular biometrics remain largely unexplored. Effectively simulating such attacks requires a sophisticated morphing generation model capable of handling the complexities of uncontrolled acquisition environments while preserving fine-grained ocular features. To address this gap, we introduce **DOOMGAN**, a novel framework that encompasses landmark-driven encoding, attention-guided generation, and dynamic weighting of multi-faceted losses for optimized convergence. We evaluated DOOMGAN against visible ocular recognition systems, achieving over 20% improvement in attack success rates over baseline models. Moreover, DOOMGAN shows a 20% boost in generating elliptical iris structures and a 30% boost in maintaining gaze consistency. Additionally, we provide the first comprehensive ocular morphing datasets to aid research in defending against such attacks.

---

## Key Contributions

-   **Novel Ocular Landmark Generator**: We introduce a landmark generator that surpasses existing models in precision, crucial for accurately morphing anatomical structures and ensuring robustness against gaze deviations.
-   **Specialized Ocular Morphing Model (DOOMGAN)**: A novel GAN-based framework designed specifically for synthesizing high-quality ocular morphs using landmark-driven encoding and attention-guided generation.
-   **Dynamic Weighting Scheme**: An innovative dynamic process to balance the contributions of a multi-faceted loss function, facilitating simultaneous and stable convergence during training.
-   **Novel Performance Metrics & Vulnerability Assessment**: We propose new ocular-specific metrics (Iris Irregularity and Gaze Direction) and perform a thorough vulnerability assessment against existing ocular recognition and morph attack detection (MAD) systems.
-   **First Public Ocular Morph Datasets**: We contribute extensive visible-light morphed ocular image datasets to facilitate future research in detecting and defending against these advanced attacks.

## Architecture Overview

DOOMGAN's architecture comprises five key components: a **Landmark Generator ($L_g$)**, a **Landmark Encoder ($L_e$)**, an **Image Encoder with Landmark Heatmaps ($E$)**, an **Attention-Based Landmark Guided Generator ($G$)**, and a **Spectral Normalization Powered Discriminator ($D$)**.

<p align="center">
  <img src="assets/Intro GAN.jpg" width="800" alt="DOOMGAN Architecture">
  <br>
  <em>Figure 2: The overall architecture of the proposed DOOMGAN for visible spectrum ocular morph generation.</em>
</p>

## Results

DOOMGAN consistently outperforms state-of-the-art landmark-based and GAN-based morphing models across all key metrics, including image quality (SSIM), anatomical correctness (IR, Gaze), and vulnerability (MMPMR, FMMPMR).

| Method             | SSIM          | IR              | Gaze            | MMPMR (0.01%) | FMMPMR (0.01%) |
| ------------------ | ------------- | --------------- | --------------- | ------------- | -------------- |
| LM (Baseline)      | **0.72**      | 0.7328          | 0.5916          | **92.50**     | 15.32          |
| MorGAN             | 0.46          | 0.5938          | 0.4716          | 3.92          | 0.01           |
| StyleGAN           | 0.47          | 0.6683          | 0.5080          | 4.60          | 0.11           |
| VAE-GAN            | 0.56          | 0.7099          | 0.6548          | 26.38         | 0.23           |
| MIPGAN             | 0.46          | 0.7723          | 0.5753          | 28.22         | 2.39           |
| MorCode            | 0.54          | 0.7826          | 0.7577          | 32.42         | 3.84           |
| **DOOMGAN (Ours)** | 0.67          | **0.9380**      | **0.9775**      | 75.00         | **30.95**      |

*Table 1: Comparative evaluation against baseline models on the VISOB dataset with the OVS-I system. DOOMGAN shows superior anatomical correctness and a significantly higher fully-mated attack rate (FMMPMR).*

### Qualitative Results

<p align="center">
  <img src="assets/Final_Morph_Compare.jpg" width="800" alt="Qualitative Comparison">
  <br>
  <em>Figure 3: Qualitative comparison showing DOOMGAN's superior ability to handle uncontrolled environments (off-angle iris, pose variation) compared to baseline methods.</em>
</p>

---

## Installation

Follow these instructions to set up a dedicated environment for running this project, ensuring all dependencies are handled correctly.

### Prerequisites

*   **Anaconda or Miniconda:** You must have a working installation of Conda.
*   **Git:** To clone the repository.
*   **(Optional) NVIDIA GPU:** For significantly faster training and inference.

### Step-by-Step Setup

1.  **Clone the Repository**

    Open your terminal or command prompt and clone this project:
    ```bash
    git clone https://github.com/Bharath-K3/DOOMGAN.git
    cd DOOMGAN
    ```

2.  **Create and Activate the Conda Environment**

    We will create a new Conda environment named `doomgan` with Python 3.10 to ensure consistency.

    ```bash
    # Create the environment named 'doomgan' with Python 3.10
    conda create -n doomgan python=3.10 -y

    # Activate the new environment
    conda activate doomgan
    ```
    Your terminal prompt should now begin with `(doomgan)`.

3.  **Install Dependencies**

    With the `doomgan` environment active, install all required libraries using the provided `requirements.txt` file.

    ```bash
    pip install -r requirements.txt
    ```

4.  **Download Pre-trained Models**

    This project requires pre-trained model weights to function. Please download the following and place them in the correct directories as specified in `config/config.yaml`:

    *   **Your trained GAN models** (Generator, Encoder, etc.) into their respective output directories (e.g., `generator_models/`).
    *   **The pre-trained ArcFace model** (for identity loss during training).
    *   **The pre-trained Landmark Predictor model** (for the web applications and generation scripts).

    **Option 1: Manual Download (File-by-File)**

    You can download the necessary model files individually from the repository's main branch and place them in the correct directories.

    Go to the Following Link:

    ```bash
    https://huggingface.co/BharathK333/DOOMGAN/tree/main
    ```

    And place the models in the following structure within the project:

    ```bash
    g_epoch_450.pth -> place in generator_models/
    d_epoch_450.pth -> place in discriminator_models/
    e_epoch_450.pth -> place in encoder_models/
    le_epoch_450.pth -> place in landmark_encoder_models/
    resnet50_arcface.pth -> place in trained_models/
    Ocular_LM_Generator.pth -> place in trained_models/
    ```

    **Option 2: Automatic Download via Git LFS (Recommended)**

    **Step 1: Prepare Your Directory**

    This is the easiest method to download all required models directly into the correct folder structure.
    ```bash
    generator_models/
    discriminator_models/
    encoder_models/
    landmark_encoder_models/
    trained_models/
    ```

    **Step 2: Install Git LFS and Clone the Repository**

    ```bash
    git lfs install
    git clone https://huggingface.co/BharathK333/DOOMGAN .
    ```

    This will download all the model files and automatically place them in the correct directories.
    We provide multiple checkpoints for each. The users can pick the best preferred ones. 
    We have initialized the defaults of this workflow to epoch 450, which the users can modify if required.
    
    
5. **Project Structure After Setup**

    After downloading the models using either method, the project directory should look like this:

    ```bash
    DOOMGAN/
    ├── assets/
    │   └── ...
    ├── config/
    │   └── config.yaml
    ├── data/
    │   └── landmarks_GAN.json
    ├── discriminator_models/
    │   └── d_epoch_450.pth
    │   └── ... (and other discriminator models)
    ├── encoder_models/
    │   └── e_epoch_450.pth
    │   └── ... (and other encoder models)
    ├── generator_models/
    │   └── g_epoch_450.pth
    │   └── ... (and other generator models)
    ├── landmark_encoder_models/
    │   └── le_epoch_450.pth
    │   └── ... (and other landmark encoder models)
    ├── models/
    │   ├── __init__.py
    │   ├── landmark_predictor.py
    │   └── ...
    ├── trained_models/
    │   ├── Ocular_LM_Generator.pth
    │   └── resnet50_arcface.pth
    ├── app.py
    ├── generate_morphs.py
    ├── train.py
    ├── README.md
    └── requirements.txt
    ```

---

## Usage

### 1. Training the Model

Before training, configure the dataset paths and hyperparameters in `config/config.yaml`. Log in to your Weights & Biases account for experiment tracking.

```bash
# Login to W&B (optional but recommended)
wandb login

# Run the training script
python train.py
```

### 2. Generating Morphs via Command Line
Use the generate_morphs.py script to create a morphed image from two source images.

```bash
python generate_morphs.py --image1 "path/to/your/image1.png" --image2 "path/to/your/image2.png" --epoch 500 --output "generated_morphs/output.png"
```

Below is an example run

```bash
python generate_morphs.py --image1 "assets/1144_r_1.png" --image2 "assets/1147_r_1.png" --epoch 450 --output "generated_morphs/1144_1147.png"
```

### 3. Running the Web Applications
We provide two interactive web applications for easy demonstration.

```bash
To launch the Streamlit app:
streamlit run app.py

To launch the Gradio app:
python app.py
```

## Citation
If you find our work useful, please consider citing our paper:
```bib
@article{krishnamurthy2025doomgan,
  title={DOOMGAN: High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing},
  author={Krishnamurthy, Bharath and Rattani, Ajita},
  journal={arXiv preprint arXiv:2507.17158},
  year={2025}
}
````

## Acknowledgements
This work is supported in part by the National Science Foundation (NSF) , United States award no. 2345561.
![](assets/NSF_Logo.png)